{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EwYlTBD2DWVx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from keras.datasets import mnist\n",
        "\n",
        "def preprocess_data(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Normalize and reshape dataset.\n",
        "    \"\"\"\n",
        "    X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalize\n",
        "    X_train, X_test = X_train.reshape(-1, 784), X_test.reshape(-1, 784)  # Flatten images\n",
        "\n",
        "    # One-hot encoding\n",
        "    y_train_encoded = np.zeros((y_train.size, 10))\n",
        "    y_train_encoded[np.arange(y_train.size), y_train] = 1\n",
        "\n",
        "    y_test_encoded = np.zeros((y_test.size, 10))\n",
        "    y_test_encoded[np.arange(y_test.size), y_test] = 1\n",
        "\n",
        "    return X_train, y_train_encoded, X_test, y_test_encoded\n",
        "\n",
        "def evaluate(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate model accuracy.\n",
        "    \"\"\"\n",
        "    y_pred = model.forward(X_test)\n",
        "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "    y_true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "    accuracy = np.mean(y_pred_labels == y_true_labels)\n",
        "    return accuracy\n",
        "\n",
        "def calculate_loss(y_true, y_pred, loss_type):\n",
        "  if loss_type == 'mse':\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "  elif loss_type == 'cross_entropy':\n",
        "    #Small epsilon to prevent log(0)\n",
        "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    return -np.mean(y_true * np.log(y_pred))\n",
        "\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "  predicted_classes = np.argmax(y_pred, axis=1)\n",
        "  true_classes = np.argmax(y_true, axis=1)\n",
        "  return np.mean(predicted_classes == true_classes)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_layers, hidden_size, output_size, activation=\"ReLU\", init_method = \"random\", loss_type = \"mse\"):\n",
        "        self.layers = [input_size] + [hidden_size] * hidden_layers + [output_size]\n",
        "        self.init_method = init_method\n",
        "        self.weights = self.initialize_weights()\n",
        "        self.biases = [np.zeros((1, self.layers[i+1])) for i in range(len(self.layers)-1)]\n",
        "        self.activation = activation\n",
        "        self.loss_type = loss_type\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        weights = []\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            if self.init_method == \"Xavier\":\n",
        "                limit = np.sqrt(1 / self.layers[i])\n",
        "                weights.append(np.random.uniform(-limit, limit, (self.layers[i], self.layers[i+1])))\n",
        "            else:\n",
        "                weights.append(np.random.randn(self.layers[i], self.layers[i+1]) * 0.01)\n",
        "        return weights\n",
        "\n",
        "    def activation_func(self, x, is_output=False):\n",
        "        if is_output and self.loss_type == \"cross_entropy\":\n",
        "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "            return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "        if self.activation == \"ReLU\":\n",
        "            return np.maximum(0, x)\n",
        "        elif self.activation == \"sigmoid\":\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "        elif self.activation == \"tanh\":\n",
        "            return np.tanh(x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.a = [X]\n",
        "        for i, (W, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            z = np.dot(self.a[-1], W) + b\n",
        "            # Check if this is the output layer\n",
        "            is_output = (i == len(self.weights) - 1)\n",
        "            self.a.append(self.activation_func(z, is_output=is_output))\n",
        "        return self.a[-1]"
      ],
      "metadata": {
        "id": "6nE3MIrdDiNE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Backpropagation:\n",
        "    def __init__(self, model, optimizer=\"sgd\", learning_rate=0.01, momentum=0.9, beta=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the backpropagation optimizer.\n",
        "        :param model: NeuralNetwork model instance\n",
        "        :param optimizer: Optimization algorithm ('sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam')\n",
        "        :param learning_rate: Learning rate for weight updates\n",
        "        :param momentum: Momentum factor for momentum-based optimizers\n",
        "        :param beta: Decay factor for RMSprop\n",
        "        :param beta1: First moment decay rate (Adam, Nadam)\n",
        "        :param beta2: Second moment decay rate (Adam, Nadam)\n",
        "        :param epsilon: Small value to prevent division by zero\n",
        "        :param weight_decay: L2 Regularisation\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.lr = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.beta = beta\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        # Initialize velocity (for Momentum and Nesterov)\n",
        "        self.velocities = [np.zeros_like(W) for W in model.weights]\n",
        "\n",
        "        # Initialize moving averages (for Adam, Nadam, RMSprop)\n",
        "        self.m = [np.zeros_like(W) for W in model.weights]\n",
        "        self.v = [np.zeros_like(W) for W in model.weights]\n",
        "        self.t = 0  # Time step for Adam/Nadam\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Perform backward pass to compute gradients.\n",
        "        :param y_true: True labels\n",
        "        :param y_pred: Predicted output\n",
        "        :return: Gradients for each layer\n",
        "        \"\"\"\n",
        "        gradients = []\n",
        "        delta = y_pred - y_true #Same for mse and cross entropy as softmax outputs is handled in feedforward\n",
        "\n",
        "        for i in reversed(range(len(self.model.weights))):\n",
        "            grad_W = np.dot(self.model.a[i].T, delta) / y_true.shape[0]\n",
        "            grad_W += self.weight_decay * self.model.weights[i]  # Apply L2 regularization\n",
        "            gradients.insert(0, grad_W)\n",
        "\n",
        "            if i > 0:\n",
        "                if self.model.activation == \"ReLU\":\n",
        "                    delta = np.dot(delta, self.model.weights[i].T) * (self.model.a[i] > 0)\n",
        "                elif self.model.activation == \"sigmoid\":\n",
        "                    delta = np.dot(delta, self.model.weights[i].T) * (self.model.a[i] * (1 - self.model.a[i]))\n",
        "                elif self.model.activation == \"tanh\":\n",
        "                    delta = np.dot(delta, self.model.weights[i].T) * (1 - self.model.a[i]**2)\n",
        "\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def update_weights(self, gradients):\n",
        "        \"\"\"\n",
        "        Updates the model weights using the selected optimizer.\n",
        "        :param gradients: List of gradient matrices for each layer\n",
        "        \"\"\"\n",
        "        self.t += 1  # Increment time step for Adam/Nadam\n",
        "\n",
        "        for i in range(len(self.model.weights)):\n",
        "            grad = gradients[i] + self.weight_decay * self.model.weights[i]  # Apply L2 regularization\n",
        "\n",
        "            if self.optimizer == \"sgd\":\n",
        "                # Stochastic Gradient Descent (SGD)\n",
        "                self.model.weights[i] -= self.lr * grad\n",
        "\n",
        "            elif self.optimizer == \"momentum\":\n",
        "                # Momentum-Based Gradient Descent\n",
        "                self.velocities[i] = self.momentum * self.velocities[i] - self.lr * grad\n",
        "                self.model.weights[i] += self.velocities[i]\n",
        "\n",
        "            elif self.optimizer == \"nesterov\":\n",
        "                # Nesterov Accelerated Gradient Descent\n",
        "                prev_velocity = self.velocities[i]\n",
        "                self.velocities[i] = self.momentum * self.velocities[i] - self.lr * grad\n",
        "                self.model.weights[i] += -self.momentum * prev_velocity + (1 + self.momentum) * self.velocities[i]\n",
        "\n",
        "            elif self.optimizer == \"rmsprop\":\n",
        "                # RMSprop\n",
        "                self.v[i] = self.beta * self.v[i] + (1 - self.beta) * (grad ** 2)\n",
        "                self.model.weights[i] -= self.lr * grad / (np.sqrt(self.v[i]) + self.epsilon)\n",
        "\n",
        "            elif self.optimizer == \"adam\":\n",
        "                # Adam (Adaptive Moment Estimation)\n",
        "                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)\n",
        "\n",
        "                # Bias correction\n",
        "                m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
        "                v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "                self.model.weights[i] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "            elif self.optimizer == \"nadam\":\n",
        "                # Nadam (Nesterov-accelerated Adam)\n",
        "                m_hat = (self.beta1 * self.m[i] + (1 - self.beta1) * grad) / (1 - self.beta1 ** self.t)\n",
        "                v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "                self.model.weights[i] -= self.lr * (self.beta1 * m_hat + (1 - self.beta1) * grad / (1 - self.beta1 ** self.t)) / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Optimizer '{self.optimizer}' is not recognized.\")\n"
      ],
      "metadata": {
        "id": "lECjxEyyDkzW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Best model configuration with val_accuracy=86.68%\n",
        "config1 = {\n",
        "    \"epochs\":10,\n",
        "    \"activation\":\"sigmoid\",\n",
        "    \"batch_size\":16,\n",
        "    \"hidden_layers\":4,\n",
        "    \"hidden_size\":64,\n",
        "    \"learning_rate\":0.001,\n",
        "    \"optimizer\":\"adam\",\n",
        "    \"weights\":\"Xavier\",\n",
        "    \"weight_decay\":0,\n",
        "    \"loss_type\":\"cross_entropy\"\n",
        "}"
      ],
      "metadata": {
        "id": "BCJOMGWEDntx"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config2 = {\n",
        "    \"epochs\":5,\n",
        "    \"activation\":\"tanh\",\n",
        "    \"batch_size\":32,\n",
        "    \"hidden_layers\":3,\n",
        "    \"hidden_size\":64,\n",
        "    \"learning_rate\":0.001,\n",
        "    \"optimizer\":\"adam\",\n",
        "    \"weights\":\"Xavier\",\n",
        "    \"weight_decay\":0.0005,\n",
        "    \"loss_type\":\"cross_entropy\"\n",
        "}"
      ],
      "metadata": {
        "id": "BOnC32IDDxM8"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config3 = {\n",
        "    \"epochs\":5,\n",
        "    \"activation\":\"ReLU\",\n",
        "    \"batch_size\":16,\n",
        "    \"hidden_layers\":3,\n",
        "    \"hidden_size\":128,\n",
        "    \"learning_rate\":0.001,\n",
        "    \"optimizer\":\"rmsprop\",\n",
        "    \"weights\":\"Xavier\",\n",
        "    \"weight_decay\":0,\n",
        "    \"loss_type\":\"mse\"\n",
        "}"
      ],
      "metadata": {
        "id": "bs0IMaMVDzw_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(config):\n",
        "  (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "  X_train, y_train, X_test, y_test = preprocess_data(X_train, y_train, X_test, y_test)\n",
        "\n",
        "  val_size = int(0.1 * len(X_train))\n",
        "  X_val, y_val = X_train[:val_size], y_train[:val_size]\n",
        "  X_train, y_train = X_train[val_size:], y_train[val_size:]\n",
        "\n",
        "  model = NeuralNetwork(input_size=784, hidden_layers=config[\"hidden_layers\"], hidden_size=config[\"hidden_size\"], output_size=10, activation=config[\"activation\"], init_method=config[\"weights\"], loss_type = config[\"loss_type\"])\n",
        "  backprop = Backpropagation(model, optimizer=config[\"optimizer\"], learning_rate=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "  for epoch in range(config[\"epochs\"]):\n",
        "    loss = 0\n",
        "    for i in range(0, len(X_train), config[\"batch_size\"]):\n",
        "      batch_X = X_train[i:i + config[\"batch_size\"]]\n",
        "      batch_y = y_train[i:i + config[\"batch_size\"]]\n",
        "\n",
        "      # Forward & Backward pass\n",
        "      y_pred = model.forward(batch_X)\n",
        "      gradients = backprop.backward(batch_y, y_pred)\n",
        "      backprop.update_weights(gradients)\n",
        "\n",
        "      # Compute loss\n",
        "      loss += calculate_loss(batch_y, y_pred, config[\"loss_type\"])\n",
        "\n",
        "    loss /= len(X_train)\n",
        "\n",
        "\n",
        "    val_pred = model.forward(X_val)\n",
        "    val_loss = calculate_loss(y_val, val_pred, config[\"loss_type\"])\n",
        "    val_accuracy = calculate_accuracy(y_val, val_pred)\n",
        "\n",
        "  #Test Model\n",
        "  test_pred = model.forward(X_test)\n",
        "  test_accuracy = calculate_accuracy(y_test, test_pred)\n",
        "  print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "D-e-3VypE--i"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train(config1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTVVWi-6FnCm",
        "outputId": "8c991624-8693-42f6-d733-52f5a72b991a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Test Accuracy: 0.9527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(config2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooBtHvZ_FpkM",
        "outputId": "7db789ac-f712-495e-b562-783d8af427ad"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9641\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(config3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx-6qJ_1FrVd",
        "outputId": "13109dfa-a359-4558-ec9b-01b1f5c43ae6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9679\n"
          ]
        }
      ]
    }
  ]
}