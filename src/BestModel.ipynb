{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sGS2-YCQ2-Ci"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 208
        },
        "id": "EZqtrNK43IiZ",
        "outputId": "6036d7e6-1e6e-4da6-8d95-b954aa4bc27c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mda24m027\u001b[0m (\u001b[33mda24m027-indian-institute-of-technology-madras\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
        "from keras.datasets import fashion_mnist\n",
        "\n",
        "def preprocess_data(X_train, y_train, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Normalize and reshape dataset.\n",
        "    \"\"\"\n",
        "    X_train, X_test = X_train / 255.0, X_test / 255.0  # Normalize\n",
        "    X_train, X_test = X_train.reshape(-1, 784), X_test.reshape(-1, 784)  # Flatten images\n",
        "\n",
        "    # One-hot encoding\n",
        "    y_train_encoded = np.zeros((y_train.size, 10))\n",
        "    y_train_encoded[np.arange(y_train.size), y_train] = 1\n",
        "\n",
        "    y_test_encoded = np.zeros((y_test.size, 10))\n",
        "    y_test_encoded[np.arange(y_test.size), y_test] = 1\n",
        "\n",
        "    return X_train, y_train_encoded, X_test, y_test_encoded\n",
        "\n",
        "def evaluate(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate model accuracy.\n",
        "    \"\"\"\n",
        "    y_pred = model.forward(X_test)\n",
        "    y_pred_labels = np.argmax(y_pred, axis=1)\n",
        "    y_true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "    accuracy = np.mean(y_pred_labels == y_true_labels)\n",
        "    return accuracy\n",
        "\n",
        "def calculate_loss(y_true, y_pred, loss_type):\n",
        "  if loss_type == 'mse':\n",
        "    return np.mean(np.square(y_true - y_pred))\n",
        "  elif loss_type == 'cross_entropy':\n",
        "    #Small epsilon to prevent log(0)\n",
        "    y_pred = np.clip(y_pred, 1e-7, 1 - 1e-7)\n",
        "    return -np.mean(y_true * np.log(y_pred))\n",
        "\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "  predicted_classes = np.argmax(y_pred, axis=1)\n",
        "  true_classes = np.argmax(y_true, axis=1)\n",
        "  return np.mean(predicted_classes == true_classes)"
      ],
      "metadata": {
        "id": "C3A26IS63O9s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_layers, hidden_size, output_size, activation=\"ReLU\", init_method = \"random\", loss_type = \"mse\"):\n",
        "        self.layers = [input_size] + [hidden_size] * hidden_layers + [output_size]\n",
        "        self.init_method = init_method\n",
        "        self.weights = self.initialize_weights()\n",
        "        self.biases = [np.zeros((1, self.layers[i+1])) for i in range(len(self.layers)-1)]\n",
        "        self.activation = activation\n",
        "        self.loss_type = loss_type\n",
        "\n",
        "    def initialize_weights(self):\n",
        "        weights = []\n",
        "        for i in range(len(self.layers) - 1):\n",
        "            if self.init_method == \"Xavier\":\n",
        "                limit = np.sqrt(1 / self.layers[i])\n",
        "                weights.append(np.random.uniform(-limit, limit, (self.layers[i], self.layers[i+1])))\n",
        "            else:\n",
        "                weights.append(np.random.randn(self.layers[i], self.layers[i+1]) * 0.01)\n",
        "        return weights\n",
        "\n",
        "    def activation_func(self, x, is_output=False):\n",
        "        if is_output and self.loss_type == \"cross_entropy\":\n",
        "            exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "            return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "        if self.activation == \"ReLU\":\n",
        "            return np.maximum(0, x)\n",
        "        elif self.activation == \"sigmoid\":\n",
        "            return 1 / (1 + np.exp(-x))\n",
        "        elif self.activation == \"tanh\":\n",
        "            return np.tanh(x)\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.a = [X]\n",
        "        for i, (W, b) in enumerate(zip(self.weights, self.biases)):\n",
        "            z = np.dot(self.a[-1], W) + b\n",
        "            # Check if this is the output layer\n",
        "            is_output = (i == len(self.weights) - 1)\n",
        "            self.a.append(self.activation_func(z, is_output=is_output))\n",
        "        return self.a[-1]"
      ],
      "metadata": {
        "id": "KhFwACjW3i7-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class Backpropagation:\n",
        "    def __init__(self, model, optimizer=\"sgd\", learning_rate=0.01, momentum=0.9, beta=0.9, beta1=0.9, beta2=0.999, epsilon=1e-8, weight_decay=0.0):\n",
        "        \"\"\"\n",
        "        Initialize the backpropagation optimizer.\n",
        "        :param model: NeuralNetwork model instance\n",
        "        :param optimizer: Optimization algorithm ('sgd', 'momentum', 'nesterov', 'rmsprop', 'adam', 'nadam')\n",
        "        :param learning_rate: Learning rate for weight updates\n",
        "        :param momentum: Momentum factor for momentum-based optimizers\n",
        "        :param beta: Decay factor for RMSprop\n",
        "        :param beta1: First moment decay rate (Adam, Nadam)\n",
        "        :param beta2: Second moment decay rate (Adam, Nadam)\n",
        "        :param epsilon: Small value to prevent division by zero\n",
        "        :param weight_decay: L2 Regularisation\n",
        "        \"\"\"\n",
        "        self.model = model\n",
        "        self.optimizer = optimizer\n",
        "        self.lr = learning_rate\n",
        "        self.momentum = momentum\n",
        "        self.beta = beta\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.weight_decay = weight_decay\n",
        "\n",
        "        # Initialize velocity (for Momentum and Nesterov)\n",
        "        self.velocities = [np.zeros_like(W) for W in model.weights]\n",
        "\n",
        "        # Initialize moving averages (for Adam, Nadam, RMSprop)\n",
        "        self.m = [np.zeros_like(W) for W in model.weights]\n",
        "        self.v = [np.zeros_like(W) for W in model.weights]\n",
        "        self.t = 0  # Time step for Adam/Nadam\n",
        "\n",
        "    def backward(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Perform backward pass to compute gradients.\n",
        "        :param y_true: True labels\n",
        "        :param y_pred: Predicted output\n",
        "        :return: Gradients for each layer\n",
        "        \"\"\"\n",
        "        gradients = []\n",
        "        delta = y_pred - y_true #Same for mse and cross entropy as softmax outputs is handled in feedforward\n",
        "\n",
        "        for i in reversed(range(len(self.model.weights))):\n",
        "            grad_W = np.dot(self.model.a[i].T, delta) / y_true.shape[0]\n",
        "            grad_W += self.weight_decay * self.model.weights[i]  # Apply L2 regularization\n",
        "            gradients.insert(0, grad_W)\n",
        "\n",
        "            if i > 0:\n",
        "                if self.model.activation == \"ReLU\":\n",
        "                    delta = np.dot(delta, self.model.weights[i].T) * (self.model.a[i] > 0)\n",
        "                elif self.model.activation == \"sigmoid\":\n",
        "                    delta = np.dot(delta, self.model.weights[i].T) * (self.model.a[i] * (1 - self.model.a[i]))\n",
        "                elif self.model.activation == \"tanh\":\n",
        "                    delta = np.dot(delta, self.model.weights[i].T) * (1 - self.model.a[i]**2)\n",
        "\n",
        "\n",
        "        return gradients\n",
        "\n",
        "    def update_weights(self, gradients):\n",
        "        \"\"\"\n",
        "        Updates the model weights using the selected optimizer.\n",
        "        :param gradients: List of gradient matrices for each layer\n",
        "        \"\"\"\n",
        "        self.t += 1  # Increment time step for Adam/Nadam\n",
        "\n",
        "        for i in range(len(self.model.weights)):\n",
        "            grad = gradients[i] + self.weight_decay * self.model.weights[i]  # Apply L2 regularization\n",
        "\n",
        "            if self.optimizer == \"sgd\":\n",
        "                # Stochastic Gradient Descent (SGD)\n",
        "                self.model.weights[i] -= self.lr * grad\n",
        "\n",
        "            elif self.optimizer == \"momentum\":\n",
        "                # Momentum-Based Gradient Descent\n",
        "                self.velocities[i] = self.momentum * self.velocities[i] - self.lr * grad\n",
        "                self.model.weights[i] += self.velocities[i]\n",
        "\n",
        "            elif self.optimizer == \"nesterov\":\n",
        "                # Nesterov Accelerated Gradient Descent\n",
        "                prev_velocity = self.velocities[i]\n",
        "                self.velocities[i] = self.momentum * self.velocities[i] - self.lr * grad\n",
        "                self.model.weights[i] += -self.momentum * prev_velocity + (1 + self.momentum) * self.velocities[i]\n",
        "\n",
        "            elif self.optimizer == \"rmsprop\":\n",
        "                # RMSprop\n",
        "                self.v[i] = self.beta * self.v[i] + (1 - self.beta) * (grad ** 2)\n",
        "                self.model.weights[i] -= self.lr * grad / (np.sqrt(self.v[i]) + self.epsilon)\n",
        "\n",
        "            elif self.optimizer == \"adam\":\n",
        "                # Adam (Adaptive Moment Estimation)\n",
        "                self.m[i] = self.beta1 * self.m[i] + (1 - self.beta1) * grad\n",
        "                self.v[i] = self.beta2 * self.v[i] + (1 - self.beta2) * (grad ** 2)\n",
        "\n",
        "                # Bias correction\n",
        "                m_hat = self.m[i] / (1 - self.beta1 ** self.t)\n",
        "                v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "                self.model.weights[i] -= self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "            elif self.optimizer == \"nadam\":\n",
        "                # Nadam (Nesterov-accelerated Adam)\n",
        "                m_hat = (self.beta1 * self.m[i] + (1 - self.beta1) * grad) / (1 - self.beta1 ** self.t)\n",
        "                v_hat = self.v[i] / (1 - self.beta2 ** self.t)\n",
        "\n",
        "                self.model.weights[i] -= self.lr * (self.beta1 * m_hat + (1 - self.beta1) * grad / (1 - self.beta1 ** self.t)) / (np.sqrt(v_hat) + self.epsilon)\n",
        "\n",
        "            else:\n",
        "                raise ValueError(f\"Optimizer '{self.optimizer}' is not recognized.\")\n"
      ],
      "metadata": {
        "id": "wt03QcJr3m85"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Best model configuration with val_accuracy=86.68%\n",
        "config = {\n",
        "    \"epochs\":10,\n",
        "    \"activation\":\"sigmoid\",\n",
        "    \"batch_size\":16,\n",
        "    \"hidden_layers\":4,\n",
        "    \"hidden_size\":64,\n",
        "    \"learning_rate\":0.001,\n",
        "    \"optimizer\":\"adam\",\n",
        "    \"weights\":\"Xavier\",\n",
        "    \"weight_decay\":0,\n",
        "    \"loss_type\":\"cross_entropy\"\n",
        "}"
      ],
      "metadata": {
        "id": "MkSHW4af4isu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "metadata": {
        "id": "MlJTVMQG6McH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n",
        "X_train, y_train, X_test, y_test = preprocess_data(X_train, y_train, X_test, y_test)\n",
        "\n",
        "val_size = int(0.1 * len(X_train))\n",
        "X_val, y_val = X_train[:val_size], y_train[:val_size]\n",
        "X_train, y_train = X_train[val_size:], y_train[val_size:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZnqnZqD6Tj8",
        "outputId": "58461508-81c2-4c15-c87c-f2fb128015e1"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "\u001b[1m29515/29515\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "\u001b[1m26421880/26421880\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "\u001b[1m5148/5148\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "\u001b[1m4422102/4422102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.init(project=\"DA6401_Assignment1\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "_-6nTtCh7xKn",
        "outputId": "152d92f5-0fe5-4db3-c72a-48416d623cd4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.7"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250308_084636-lfv44p0s</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/da24m027-indian-institute-of-technology-madras/DA6401_Assignment1/runs/lfv44p0s' target=\"_blank\">soft-river-39</a></strong> to <a href='https://wandb.ai/da24m027-indian-institute-of-technology-madras/DA6401_Assignment1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/da24m027-indian-institute-of-technology-madras/DA6401_Assignment1' target=\"_blank\">https://wandb.ai/da24m027-indian-institute-of-technology-madras/DA6401_Assignment1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/da24m027-indian-institute-of-technology-madras/DA6401_Assignment1/runs/lfv44p0s' target=\"_blank\">https://wandb.ai/da24m027-indian-institute-of-technology-madras/DA6401_Assignment1/runs/lfv44p0s</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/da24m027-indian-institute-of-technology-madras/DA6401_Assignment1/runs/lfv44p0s?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7e27ba2d0310>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = NeuralNetwork(input_size=784, hidden_layers=config[\"hidden_layers\"], hidden_size=config[\"hidden_size\"], output_size=10, activation=config[\"activation\"], init_method=config[\"weights\"], loss_type = config[\"loss_type\"])\n",
        "backprop = Backpropagation(model, optimizer=config[\"optimizer\"], learning_rate=config[\"learning_rate\"], weight_decay=config[\"weight_decay\"])\n",
        "\n",
        "for epoch in range(config[\"epochs\"]):\n",
        "  loss = 0\n",
        "  for i in range(0, len(X_train), config[\"batch_size\"]):\n",
        "    batch_X = X_train[i:i + config[\"batch_size\"]]\n",
        "    batch_y = y_train[i:i + config[\"batch_size\"]]\n",
        "\n",
        "    # Forward & Backward pass\n",
        "    y_pred = model.forward(batch_X)\n",
        "    gradients = backprop.backward(batch_y, y_pred)\n",
        "    backprop.update_weights(gradients)\n",
        "\n",
        "    # Compute loss\n",
        "    loss += calculate_loss(batch_y, y_pred, config[\"loss_type\"])\n",
        "\n",
        "  loss /= len(X_train)\n",
        "  wandb.log({\"epoch\": epoch+1, \"loss\": loss})\n",
        "\n",
        "  # Validate model\n",
        "  val_pred = model.forward(X_val)\n",
        "  val_loss = calculate_loss(y_val, val_pred, config[\"loss_type\"])\n",
        "  val_accuracy = calculate_accuracy(y_val, val_pred)\n",
        "  wandb.log({\"epoch\": epoch+1, \"val_loss\": val_loss, \"val_accuracy\": val_accuracy})\n",
        "\n",
        "#Test Model\n",
        "test_pred = model.forward(X_test)\n",
        "test_accuracy = calculate_accuracy(y_test, test_pred)\n",
        "wandb.log({\"test_accuracy\": test_accuracy})\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTBUOAM66kgy",
        "outputId": "38fdaf29-13aa-4f0d-c307-e1ff0710f51e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.8617\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes = np.argmax(test_pred, axis=1)\n",
        "true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "cm = confusion_matrix(true_classes, predicted_classes)\n",
        "labels = [\"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\",\n",
        "               \"Sandal\", \"Shirt\", \"Sneaker\", \"Bag\", \"Ankle boot\"]\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "sns.heatmap(cm, annot=False, fmt=\"d\", cmap=\"magma\", cbar=False, linewidths=1, linecolor=\"white\",\n",
        "            xticklabels=labels, yticklabels=labels, ax=ax)\n",
        "\n",
        "ax.set_xticks([])\n",
        "ax.set_yticks([])\n",
        "\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "wandb.log({\"Confusion_Matrix\": wandb.Image(fig)})\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "Z69OlE0F8mxV"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "\n",
        "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"coolwarm\", cbar=True,\n",
        "            linewidths=2, linecolor=\"black\", square=True,\n",
        "            xticklabels=labels, yticklabels=labels, alpha=0.85)\n",
        "\n",
        "plt.xlabel(\"Predicted\", fontsize=14, fontweight=\"bold\", color=\"black\")\n",
        "plt.ylabel(\"True\", fontsize=14, fontweight=\"bold\", color=\"black\")\n",
        "plt.title(\"Confusion Matrix\", fontsize=16, fontweight=\"bold\", color=\"black\")\n",
        "\n",
        "plt.xticks(color=\"black\", fontsize=12)\n",
        "plt.yticks(color=\"black\", fontsize=12)\n",
        "\n",
        "wandb.log({\"Best Model Confusion Matrix\": wandb.Image(fig)})\n",
        "plt.close(fig)"
      ],
      "metadata": {
        "id": "CDq3gVaL-dOy"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.run.finish()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        },
        "id": "_3xiTpQXAIDZ",
        "outputId": "163b99ca-0012-49f5-f03a-7f4f61bcd424"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>loss</td><td>▆█▄▃▂▂▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>val_accuracy</td><td>▅▁▅▆▇██████</td></tr><tr><td>val_loss</td><td>▅█▄▃▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>10</td></tr><tr><td>loss</td><td>0.00189</td></tr><tr><td>test_accuracy</td><td>0.8617</td></tr><tr><td>val_accuracy</td><td>0.87167</td></tr><tr><td>val_loss</td><td>0.03807</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">soft-river-39</strong> at: <a href='https://wandb.ai/da24m027-indian-institute-of-technology-madras/DA6401_Assignment1/runs/lfv44p0s' target=\"_blank\">https://wandb.ai/da24m027-indian-institute-of-technology-madras/DA6401_Assignment1/runs/lfv44p0s</a><br> View project at: <a href='https://wandb.ai/da24m027-indian-institute-of-technology-madras/DA6401_Assignment1' target=\"_blank\">https://wandb.ai/da24m027-indian-institute-of-technology-madras/DA6401_Assignment1</a><br>Synced 5 W&B file(s), 4 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250308_084636-lfv44p0s/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}